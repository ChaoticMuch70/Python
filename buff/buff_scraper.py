import json
import os
import time
import re
from datetime import datetime
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import pandas as pd
from tqdm import tqdm

COOKIE_FILE = "buff_cookies.json"

def load_cookies(context):
    if not os.path.exists(COOKIE_FILE):
        print("âš ï¸ æœªæ‰¾åˆ° Cookieï¼Œè¯·å…ˆç™»å½•...")
        return False

    with open(COOKIE_FILE, "r", encoding="utf-8") as f:
        try:
            cookies = json.load(f)
            context.add_cookies(cookies)
            print("âœ… å·²åŠ è½½æœ¬åœ° Cookie")
            return True
        except Exception as e:
            print("âŒ Cookie åŠ è½½å¤±è´¥:", e)
            return False

def save_login_cookies(context):
    print("ğŸ”‘ è¯·ç™»å½•åæŒ‰ä»»æ„é”®ç»§ç»­...")
    input()
    cookies = context.cookies()
    with open(COOKIE_FILE, "w", encoding="utf-8") as f:
        json.dump(cookies, f, ensure_ascii=False, indent=2)
    print("âœ… Cookie å·²ä¿å­˜")

def get_max_page(content):
    soup = BeautifulSoup(content, "html.parser")
    pages = soup.select("a.page-link[href]")
    max_page = 1
    for page in pages:
        href = page.get("href")
        if href and "page_num=" in href:
            try:
                num = int(href.split("page_num=")[-1])
                if num > max_page:
                    max_page = num
            except:
                pass
    print(f"ğŸ“„ ç¡®è®¤æ€»é¡µæ•°ï¼š{max_page}")
    return 10

def parse_items(content):
    soup = BeautifulSoup(content, "html.parser")
    items = []
    for li in soup.select("ul.card_csgo > li"):
        try:
            name = li.select_one("h3 a").text.strip()

            price_tag = li.select_one("strong.f_Strong")
            price_text = price_tag.text.strip().replace("Â¥", "").replace(",", "")  # å»é™¤ï¿¥å’Œé€—å·

            # ç”¨æ­£åˆ™æ‰¾ä»·æ ¼æ•°å­—ï¼Œä¾‹å¦‚åŒ¹é… '19.24', '3.8' ç­‰æ ¼å¼
            match = re.search(r"\d+(\.\d+)?", price_text)
            if match:
                price = float(match.group())
            else:
                raise ValueError(f"æ— æ³•è§£æä»·æ ¼: '{price_text}'")

            amount_text = li.select_one("span.l_Right").text.strip()
            amount = int("".join(filter(str.isdigit, amount_text)))

            items.append({"åç§°": name, "æœ€ä½ä»·": price, "åœ¨å”®æ•°é‡": amount})

        except Exception as e:
            print(f"âš ï¸ è§£æé”™è¯¯ï¼š{e}")
    return items


def scrape_buff_items():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        context = browser.new_context()
        page = context.new_page()

        if not load_cookies(context):
            page.goto("https://buff.163.com/market/csgo#tab=selling")
            save_login_cookies(context)
        else:
            page.goto("https://buff.163.com/market/csgo#tab=selling")
            page.wait_for_timeout(3000)

        content = page.content()
        max_pages = get_max_page(content)

        data = []

        for i in tqdm(range(1, max_pages + 1), desc="æŠ“å–ä¸­"):
            url = f"https://buff.163.com/market/csgo#tab=selling&page_num={i}"
            page.goto(url)
            page.wait_for_timeout(2000)
            content = page.content()
            items = parse_items(content)
            if items:
                data.extend(items)
            else:
                print(f"âš ï¸ ç¬¬ {i} é¡µæ²¡æœ‰æŠ“åˆ°æ•°æ®")

        browser.close()

        if data:
            df = pd.DataFrame(data)
            filename = f"buff_csgo_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            df.to_csv(filename, index=False, encoding="utf-8-sig")
            print(f"âœ… å…±æŠ“å–é¥°å“æ•°ï¼š{len(data)}ï¼Œæ•°æ®å·²ä¿å­˜ä¸º {filename}")
        else:
            print("âš ï¸ æ²¡æœ‰æŠ“åˆ°ä»»ä½•é¥°å“æ•°æ®")

if __name__ == "__main__":
    scrape_buff_items()
